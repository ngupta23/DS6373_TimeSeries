---
title: "GDP Prediction"
author: "Nikhil Gupta"
date: "`r Sys.time()`"
output:
  html_document:
    toc: yes
    toc_depth: 6
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '6'
always_allow_html: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Setup
```{r}
library(tswge)
library(tswgewrapped)
library(tidyverse)
library(ggplot2)
library(DT)
```

```{r}
data = read.csv("economic_indicators_all_ex_3mo_china.csv")
data %>% glimpse()
```

```{r}
x = data$gdp_change
```

```{r}
px = plotts.sample.wge(x)
```

## Stationarity

```{r}
tswgewrapped::check_stationarity(x)
```

### Condition 1

* Looks like there is a slight trend in the data with the mean moving down over time. This would be expected. As a country is growing, its GDP is expected to be high. As it becomes a more developed economy, the GDP settles at a lower but steadier value.
* The ACF plots shows extended autocorrelations although there is also a hint of exponentially decaying behavior. Hence, this trend (wanderig behavior) could be a result of a stationary AR process with positive phi values or it could be a result of a non-stationaty ARIMA like process. 
* **In summary, the mean is changing over time (wandering behavior) and based on the ACFs, this could be coming from either a stationary or a non stationary process.**


### Condition 2

* Since we have only 1 realization, it is hard to say whether the varaince is different at different time points. 
* However we can make some important observations from this realization and domain knowledge. We see that in the initial part of the graph there is more volatility in the GDP numbers compared to the second half of the graph. This is again expected based on domain knowledge. Just as a developing economy has a higher GDP change value per quarter in general, this comes with a higher volatility. As an economy becomes more developed, not only does the GDP settle to a lower value in general but also the volatility decreases as well.
* Given the above observations, there may be some hints that condition 2 has not been met


### Condition 3

* Both the first half and second half ACFs show a damped exponential behavior for the first few lags although the second half ACFs take longer to die down. Also, the 1st half ACF shows higher values at lags of 9, 10 and 11 compared to the second half. It is also interesting to see that neither the firs half nor the second half ACF matches the full data ACF. There is enough evience here to indicate that the data is not stationary.


### Conclusion

* Given the above analysis, there is a good chance that this data is not coming from a stationary process, although there were some hints (when looking at the mean) that it could have resulted from a stationary AR process. In order to completely eliminate the possibility that this may be coming from a stationary process, we will conduct an initial analysis with a stationary model.

## Stationary Model

### Setup

```{r}
n = length(x)
n.ahead = 2
batch_size = 50 ## 12 years to predict the next 2 quarters
# Placeholder for results with rolling Window (but could hold just 1 batch if needed)
results = tribble(~model, ~ASE, ~Time) 
```

### Model ID

```{r}
aic5.wge(x)
```

### Parameter Estimation
```{r}
est.s.mle = est.arma.wge(x, p = 2, q = 1)
```

**OBSERVATIONS**

* This clears a lot of confusion. **Even when fitting a stationary ARMA model, we get an estimate of phi = 0.9917 which is very close to 1 (non stationary)**. Hence the confusion that we had before can be cleared now. The data most definitely is coming from a non stationary process. For the sake of completeness, we will continue modeling with this stationary model and see how well it performs.

### Forecasting 

```{r}
phi = est.s.mle$phi
theta = est.s.mle$theta
```

```{r include=FALSE}
f = fore.arma.wge(x, phi=phi, theta = theta,
                  n.ahead = n.ahead, limits=FALSE, lastn = TRUE, plot = FALSE)

# Without Sliding Window (usual method)
ase = mean((x[(n-n.ahead+1):n] - f$f)^2)
ase
```

```{r}
r = sliding_ase(x, phi = phi, theta = theta, n.ahead = n.ahead)

ASEs = r$ASEs
Time = r$time

print(paste("ASE for the model (Single Batch): ", ASEs))

results = results %>% add_row(model = "ARMA_2_1_MLE_nobatch", ASE = ASEs, Time = Time) 
```

```{r include=FALSE}
if (ase != r$ASEs){
  stop("Sliding Window ASE is not the same as that obtained by regular method. Please check your code")
}

```

```{r}
r = sliding_ase(x, phi = phi, theta = theta, n.ahead = n.ahead, batch_size = batch_size)

ASEs = r$ASEs
Time = r$time

print(paste("ASE for the model (Rolling Window): ", mean(ASEs)))

results = results %>% add_row(model = "ARMA_2_1_MLE_bs50", ASE = ASEs, Time = Time) 


```



## Non Stationary Model

Next we will evaluate this as a non stationary model.

Because of the extended autocorrelations in the data, we will take the first difference and check the resultant data for stationarity

### Model ID
```{r}
dif1 = artrans.wge(x, phi.tr = 1)
px = plotts.sample.wge(dif1)
```

* ACF is indicatove of a MA(1) model with positive theta since most ACFs die down after lag = 1 and there is a dip in the Spectral Density at f = 0.


```{r}
tswgewrapped::check_stationarity(dif1)
```

```{r}
aic5.wge(dif1, type = 'aic')
```

* AIC indicates that ARMA(1,1) is the best although our initial guess of MA(1) is not far behind.

```{r}
aic5.wge(dif1, type = 'bic')
```

* With BIC, the order of the first 2 models is flipped. Now our initial guess of MA(1) takes 1st place.

We will try both to see which is the better model


### Parameter Estimation

#### ARMA(1,1)
```{r}
est.ns.mle = est.arma.wge(dif1, p = 1, q = 1)
```


```{r}
est.ns.mle$theta
```

* Theta is the dominant factor here. This is almost no impact of the AR side since the root is so far away from the unit circle.
* **Hence, we should stick to our initial assessment of an MA(1) model.**

#### MA(1)
```{r}
est.ns.mle = est.arma.wge(dif1, p = 0, q = 1)
```


```{r}
est.ns.mle$theta
```

### Forecasting


```{r}
phi = est.ns.mle$phi
theta = est.ns.mle$theta
d = 1
s = 0
```

```{r include=FALSE}
f = fore.aruma.wge(x, phi=phi, theta = theta, d = d, s = s,
                  n.ahead = n.ahead, limits=FALSE, lastn = TRUE, plot = FALSE)

# Without Sliding Window (usual method)
ase = mean((x[(n-n.ahead+1):n] - f$f)^2)
ase
```

```{r}
r = sliding_ase(x, phi = phi, theta = theta, d = d, s = s, n.ahead = n.ahead)

ASEs = r$ASEs
Time = r$time

print(paste("ASE for the model (Single Batch): ", ASEs))

results = results %>% add_row(model = "ARIMA_0_1_1_0_MLE_nobatch", ASE = ASEs, Time = Time) 
```

```{r include=FALSE}
if (ase != r$ASEs){
  stop("Sliding Window ASE is not the same as that obtained by regular method. Please check your code")
}

```

```{r}
r = sliding_ase(x, phi = phi, theta = theta, d = d, s = s, n.ahead = n.ahead, batch_size = batch_size)

ASEs = r$ASEs
Time = r$time

print(paste("ASE for the model (Rolling Window): ", mean(ASEs)))

results = results %>% add_row(model = "ARIMA_0_1_1_0_MLE_bs50", ASE = ASEs, Time = Time) 


```


## Visualizing Results

```{r}
# Final Results
DT::datatable(results)
```

```{r}
results %>% 
  group_by(model) %>% 
  summarise(ASE_mean = mean(ASE),
            ASE_median = median(ASE),
            ASE_sd = sd(ASE),
            num_batches = n())
```

```{r}
ggplot(results, aes(x = model, y = ASE, color = model)) + geom_boxplot()
```


```{r}
all_time = data.frame(Time = seq(1, length(x)),
                      model = c(rep("ARMA_2_1_MLE_bs50", length(x)), rep("ARIMA_0_1_1_0_MLE_bs50", length(x))),
                      ASE = 0) %>% 
  mutate_if(is.factor, as.character) 

results2 = left_join(all_time, results, by = c("Time", "model")) %>% 
  mutate(ASE = ASE.x + ASE.y) 

```

```{r include=FALSE}
# This is purely to check that the imputation of data for initial time points.
# This will not get printed in the knitted document.

selected_data = results2 %>%
  select(Time, model, ASE)
  
selected_data = inner_join(results, selected_data, by = c("Time", "model"))
selected_data = selected_data %>% 
  mutate(not_equal = ASE.x != ASE.y)

if (sum(selected_data$not_equal) != 0){
  stop("There was some issue in the filling of data from initial time points (0 values). Please check code.")
}
```


```{r warning=FALSE, message=FALSE}
data = data.frame(Time = seq(1, length(x)), Data = x)
g1 = ggplot(data, aes(x = Time, y = Data)) + geom_line()

g2 = ggplot(results2 %>% dplyr::filter(model %in% c("ARMA_2_1_MLE_bs50", "ARIMA_0_1_1_0_MLE_bs50")),
            aes(x = Time, y = ASE, color = model)) + 
  geom_line()

g1/g2
```

**CONCLUSION**

* It looks like both model performs pooprly in predicting severe downturns (~ time point 80, 120, 150) and upturns (~ time points 48, 127, 172).
* We may need to inclue exogenous variables into our model that are more indicative of these downturns and upturns in order to improve the model performance.


```{r}

```



